{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the package\n",
    "import urllib.request\n",
    "base_url = 'https://scholar.google.com/'\n",
    "url = \"https://scholar.google.com/citations?hl=en&user=fZKJdb0AAAAJ\"\n",
    " \n",
    "html_doc = urllib.request.urlopen(url).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "def read_page_and_get_soup(url):\n",
    "    html_doc = urllib.request.urlopen(url).read()\n",
    "    return BeautifulSoup(html_doc, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zachary Novack'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = soup.find(id='gsc_prf_in').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_layout_tags = {\n",
    "    'citation_table': 'gsc_a_b',\n",
    "    'work_info_row': 'gsc_a_t',\n",
    "    'work_info_table': 'gsc_oci_table',\n",
    "    'field_name': 'gsc_oci_field',\n",
    "    'field_value': 'gsc_oci_value'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Chils: Zero-shot image classification with hierarchical label sets', 'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:9yKSN-GCB0IC'}, {'title': 'Ditto: Diffusion inference-time t-optimization for music generation', 'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:qjMakFHDy7sC'}, {'title': 'Disentangling the Mechanisms Behind Implicit Regularization in SGD', 'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:u-x6o8ySG0sC'}, {'title': 'Futga: Towards Fine-grained Music Understanding through Temporally-enhanced Generative Augmentation', 'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:IjCSPb-OGe4C'}, {'title': 'DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation', 'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:UeHWp8X0CEIC'}, {'title': 'Presto! Distilling Steps and Layers for Accelerating Music Generation', 'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:YsMSGLbcyi4C'}, {'title': 'Deriving Representative Structure Over Music Corpora', 'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:eQOLeE2rZwMC'}, {'title': 'CoLLAP: Contrastive Long-form Language-Audio Pretraining with Musical Temporal Structure Augmentation', 'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:W7OEmFMy1HYC'}, {'title': 'PDMX: A Large-Scale Public Domain MusicXML Dataset for Symbolic Music Processing', 'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:zYLM7Y9cAGgC'}, {'title': 'Leveraging Structure and Context for Language-Adjacent Representation Learning', 'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:Y0pCki6q_DkC'}, {'title': 'Generative AI for Music and Audio', 'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:Tyk-4Ss8FVUC'}, {'title': 'Unsupervised Lead Sheet Generation via Semantic Compression', 'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:2osOgNQ5qMEC'}, {'title': 'Down the Rabbit Hole: Modeling Twitter Dynamics through Bayesian Inference', 'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:u5HHmVD_uO8C'}]\n"
     ]
    }
   ],
   "source": [
    "works = []\n",
    "for table_row in soup.find(id=page_layout_tags['citation_table']).findAll('tr', recursive=False):\n",
    "    work = {}\n",
    "    for table_column in table_row.findAll('td', recursive=False):\n",
    "        if page_layout_tags['work_info_row'] in table_column['class']:\n",
    "            work['title'] = table_column.a.text\n",
    "            work['link'] = table_column.a['href']\n",
    "    works.append(work)\n",
    "\n",
    "print(works)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_work_fields = [\n",
    "    'authors', 'publication_date', 'conference', \n",
    "    'pages', 'publisher', 'description', \n",
    "    'total_citations',\n",
    "]\n",
    "\n",
    "def default_processor(soup):\n",
    "    return soup.text\n",
    "\n",
    "def process_text_date(soup):\n",
    "    import dateutil.parser as parser\n",
    "    return parser.parse(soup.text,yearfirst=True, dayfirst=False).strftime('%Y-%m-%d')\n",
    "\n",
    "def process_authors(soup):\n",
    "    return [author.strip() for author in soup.text.split(',')]\n",
    "\n",
    "def process_citations(soup):\n",
    "    total_citations = int(soup.div.a.text.replace('Cited by ', ''))\n",
    "    return total_citations\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "field_processors = defaultdict(lambda: default_processor)\n",
    "field_processors['publication_date'] = process_text_date\n",
    "field_processors['authors'] = process_authors\n",
    "field_processors['total_citations'] = process_citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Chils: Zero-shot image classification with hierarchical label sets',\n",
       "  'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:9yKSN-GCB0IC',\n",
       "  'authors': ['Zachary Novack',\n",
       "   'Julian McAuley',\n",
       "   'Zachary Chase Lipton',\n",
       "   'Saurabh Garg'],\n",
       "  'publication_date': '2023-07-03',\n",
       "  'conference': 'International Conference on Machine Learning',\n",
       "  'pages': '26342-26362',\n",
       "  'publisher': 'PMLR',\n",
       "  'description': 'Open vocabulary models (eg CLIP) have shown strong performance on zero-shot classification through their ability generate embeddings for each class based on their (natural language) names. Prior work has focused on improving the accuracy of these models through prompt engineering or by incorporating a small amount of labeled downstream data (via finetuning). However, there has been little focus on improving the richness of the class names themselves, which can pose issues when class labels are coarsely-defined and are uninformative. We propose Classification with Hierarchical Label Sets (or CHiLS), an alternative strategy for zero-shot classification specifically designed for datasets with implicit semantic hierarchies. CHiLS proceeds in three steps:(i) for each class, produce a set of subclasses, using either existing label hierarchies or by querying GPT-3;(ii) perform the standard zero-shot CLIP procedure as though these subclasses were the labels of interest;(iii) map the predicted subclass back to its parent to produce the final prediction. Across numerous datasets with underlying hierarchical structure, CHiLS leads to improved accuracy in situations both with and without ground-truth hierarchical information. CHiLS is simple to implement within existing zero-shot pipelines and requires no additional training cost. Code is available at: https://github. com/acmi-lab/CHILS.',\n",
       "  'total_citations': 71},\n",
       " {'title': 'Ditto: Diffusion inference-time t-optimization for music generation',\n",
       "  'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:qjMakFHDy7sC',\n",
       "  'authors': ['Zachary Novack',\n",
       "   'Julian McAuley',\n",
       "   'Taylor Berg-Kirkpatrick',\n",
       "   'Nicholas J Bryan'],\n",
       "  'publication_date': '2024-01-22',\n",
       "  'description': 'We propose Diffusion Inference-Time T-Optimization (DITTO), a general-purpose frame-work for controlling pre-trained text-to-music diffusion models at inference-time via optimizing initial noise latents. Our method can be used to optimize through any differentiable feature matching loss to achieve a target (stylized) output and leverages gradient checkpointing for memory efficiency. We demonstrate a surprisingly wide-range of applications for music generation including inpainting, outpainting, and looping as well as intensity, melody, and musical structure control - all without ever fine-tuning the underlying model. When we compare our approach against related training, guidance, and optimization-based methods, we find DITTO achieves state-of-the-art performance on nearly all tasks, including outperforming comparable approaches on controllability, audio quality, and computational efficiency, thus opening the door for high-quality, flexible, training-free control of diffusion models. Sound examples can be found at https://DITTO-Music.github.io/web/.',\n",
       "  'total_citations': 16},\n",
       " {'title': 'Disentangling the Mechanisms Behind Implicit Regularization in SGD',\n",
       "  'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:u-x6o8ySG0sC',\n",
       "  'authors': ['Zachary Novack',\n",
       "   'Simran Kaur',\n",
       "   'Tanya Marwah',\n",
       "   'Saurabh Garg',\n",
       "   'Zachary C Lipton'],\n",
       "  'publication_date': '2022-11-29',\n",
       "  'description': \"A number of competing hypotheses have been proposed to explain why small-batch Stochastic Gradient Descent (SGD)leads to improved generalization over the full-batch regime, with recent work crediting the implicit regularization of various quantities throughout training. However, to date, empirical evidence assessing the explanatory power of these hypotheses is lacking. In this paper, we conduct an extensive empirical evaluation, focusing on the ability of various theorized mechanisms to close the small-to-large batch generalization gap. Additionally, we characterize how the quantities that SGD has been claimed to (implicitly) regularize change over the course of training. By using micro-batches, i.e. disjoint smaller subsets of each mini-batch, we empirically show that explicitly penalizing the gradient norm or the Fisher Information Matrix trace, averaged over micro-batches, in the large-batch regime recovers small-batch SGD generalization, whereas Jacobian-based regularizations fail to do so. This generalization performance is shown to often be correlated with how well the regularized model's gradient norms resemble those of small-batch SGD. We additionally show that this behavior breaks down as the micro-batch size approaches the batch size. Finally, we note that in this line of inquiry, positive experimental findings on CIFAR10 are often reversed on other datasets like CIFAR100, highlighting the need to test hypotheses on a wider collection of datasets.\",\n",
       "  'total_citations': 2},\n",
       " {'title': 'Futga: Towards Fine-grained Music Understanding through Temporally-enhanced Generative Augmentation',\n",
       "  'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:IjCSPb-OGe4C',\n",
       "  'authors': ['Junda Wu',\n",
       "   'Zachary Novack',\n",
       "   'Amit Namburi',\n",
       "   'Jiaheng Dai',\n",
       "   'Hao-Wen Dong',\n",
       "   'Zhouhang Xie',\n",
       "   'Carol Chen',\n",
       "   'Julian McAuley'],\n",
       "  'publication_date': '2024-07-29',\n",
       "  'description': \"Existing music captioning methods are limited to generating concise global descriptions of short music clips, which fail to capture fine-grained musical characteristics and time-aware musical changes. To address these limitations, we propose FUTGA, a model equipped with fined-grained music understanding capabilities through learning from generative augmentation with temporal compositions. We leverage existing music caption datasets and large language models (LLMs) to synthesize fine-grained music captions with structural descriptions and time boundaries for full-length songs. Augmented by the proposed synthetic dataset, FUTGA is enabled to identify the music's temporal changes at key transition points and their musical functions, as well as generate detailed descriptions for each music segment. We further introduce a full-length music caption dataset generated by FUTGA, as the augmentation of the MusicCaps and the Song Describer datasets. We evaluate the automatically generated captions on several downstream tasks, including music generation and retrieval. The experiments demonstrate the quality of the generated captions and the better performance in various downstream tasks achieved by the proposed music captioning approach. Our code and datasets can be found in \\\\href{https://huggingface.co/JoshuaW1997/FUTGA}{\\\\textcolor{blue}{https://huggingface.co/JoshuaW1997/FUTGA}}.\",\n",
       "  'total_citations': 1},\n",
       " {'title': 'DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation',\n",
       "  'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:UeHWp8X0CEIC',\n",
       "  'authors': ['Zachary Novack',\n",
       "   'Julian McAuley',\n",
       "   'Taylor Berg-Kirkpatrick',\n",
       "   'Nicholas Bryan'],\n",
       "  'publication_date': '2024-05-30',\n",
       "  'description': 'Controllable music generation methods are critical for human-centered AI-based music creation, but are currently limited by speed, quality, and control design trade-offs. Diffusion Inference-Time T-optimization (DITTO), in particular, offers state-of-the-art results, but is over 10x slower than real-time, limiting practical use. We propose Distilled Diffusion Inference-Time T -Optimization (or DITTO-2), a new method to speed up inference-time optimization-based control and unlock faster-than-real-time generation for a wide-variety of applications such as music inpainting, outpainting, intensity, melody, and musical structure control. Our method works by (1) distilling a pre-trained diffusion model for fast sampling via an efficient, modified consistency or consistency trajectory distillation process (2) performing inference-time optimization using our distilled model with one-step sampling as an efficient surrogate optimization task and (3) running a final multi-step sampling generation (decoding) using our estimated noise latents for best-quality, fast, controllable generation. Through thorough evaluation, we find our method not only speeds up generation over 10-20x, but simultaneously improves control adherence and generation quality all at once. Furthermore, we apply our approach to a new application of maximizing text adherence (CLAP score) and show we can convert an unconditional diffusion model without text inputs into a model that yields state-of-the-art text control. Sound examples can be found at https://ditto-music.github.io/ditto2/.',\n",
       "  'total_citations': 1},\n",
       " {'title': 'Presto! Distilling Steps and Layers for Accelerating Music Generation',\n",
       "  'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:YsMSGLbcyi4C',\n",
       "  'authors': ['Zachary Novack',\n",
       "   'Ge Zhu',\n",
       "   'Jonah Casebeer',\n",
       "   'Julian McAuley',\n",
       "   'Taylor Berg-Kirkpatrick',\n",
       "   'Nicholas J Bryan'],\n",
       "  'publication_date': '2024-10-07',\n",
       "  'description': 'Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) -- the fastest high-quality TTM to our knowledge. Sound examples can be found at https://presto-music.github.io/web/.'},\n",
       " {'title': 'Deriving Representative Structure Over Music Corpora',\n",
       "  'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:eQOLeE2rZwMC',\n",
       "  'authors': ['Ilana Shapiro',\n",
       "   'Ruanqianqian Huang',\n",
       "   'Zachary Novack',\n",
       "   'Cheng-I Wang',\n",
       "   'Hao-Wen Dong',\n",
       "   'Taylor Berg-Kirkpatrick',\n",
       "   'Shlomo Dubnov',\n",
       "   'Sorin Lerner'],\n",
       "  'publication_date': '2024-10-05',\n",
       "  'description': 'Western music is an innately hierarchical system of interacting levels of structure, from fine-grained melody to high-level form. In order to analyze music compositions holistically and at multiple granularities, we propose a unified, hierarchical meta-representation of musical structure called the structural temporal graph (STG). For a single piece, the STG is a data structure that defines a hierarchy of progressively finer structural musical features and the temporal relationships between them. We use the STG to enable a novel approach for deriving a representative structural summary of a music corpus, which we formalize as a dually NP-hard combinatorial optimization problem. Our approach first applies simulated annealing to develop a measure of structural distance between two music pieces rooted in graph isomorphism. Our approach then combines the formal guarantees of SMT solvers with nested simulated annealing over structural distances to produce a structurally sound, representative centroid STG for an entire corpus of STGs obtained from individual pieces. To evaluate our approach, we conduct experiments showing that structural distance accurately differentiates between music pieces, and that our computed centroids encapsulate the overarching structure of a music corpus.'},\n",
       " {'title': 'CoLLAP: Contrastive Long-form Language-Audio Pretraining with Musical Temporal Structure Augmentation',\n",
       "  'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:W7OEmFMy1HYC',\n",
       "  'authors': ['Junda Wu',\n",
       "   'Warren Li',\n",
       "   'Zachary Novack',\n",
       "   'Amit Namburi',\n",
       "   'Carol Chen',\n",
       "   'Julian McAuley'],\n",
       "  'publication_date': '2024-10-03',\n",
       "  'description': 'Modeling temporal characteristics plays a significant role in the representation learning of audio waveform. We propose Contrastive Long-form Language-Audio Pretraining (\\\\textbf{CoLLAP}) to significantly extend the perception window for both the input audio (up to 5 minutes) and the language descriptions (exceeding 250 words), while enabling contrastive learning across modalities and temporal dynamics. Leveraging recent Music-LLMs to generate long-form music captions for full-length songs, augmented with musical temporal structures, we collect 51.3K audio-text pairs derived from the large-scale AudioSet training dataset, where the average audio length reaches 288 seconds. We propose a novel contrastive learning architecture that fuses language representations with structured audio representations by segmenting each song into clips and extracting their embeddings. With an attention mechanism, we capture multimodal temporal correlations, allowing the model to automatically weigh and enhance the final fusion score for improved contrastive alignment. Finally, we develop two variants of the CoLLAP model with different types of backbone language models. Through comprehensive experiments on multiple long-form music-text retrieval datasets, we demonstrate consistent performance improvement in retrieval accuracy compared with baselines. We also show the pretrained CoLLAP models can be transferred to various music information retrieval tasks, with heterogeneous long-form multimodal contexts.'},\n",
       " {'title': 'PDMX: A Large-Scale Public Domain MusicXML Dataset for Symbolic Music Processing',\n",
       "  'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:zYLM7Y9cAGgC',\n",
       "  'authors': ['Phillip Long',\n",
       "   'Zachary Novack',\n",
       "   'Taylor Berg-Kirkpatrick',\n",
       "   'Julian McAuley'],\n",
       "  'publication_date': '2024-09-17',\n",
       "  'description': 'The recent explosion of generative AI-Music systems has raised numerous concerns over data copyright, licensing music from musicians, and the conflict between open-source AI and large prestige companies. Such issues highlight the need for publicly available, copyright-free musical data, in which there is a large shortage, particularly for symbolic music data. To alleviate this issue, we present PDMX: a large-scale open-source dataset of over 250K public domain MusicXML scores collected from the score-sharing forum MuseScore, making it the largest available copyright-free symbolic music dataset to our knowledge. PDMX additionally includes a wealth of both tag and user interaction metadata, allowing us to efficiently analyze the dataset and filter for high quality user-generated scores. Given the additional metadata afforded by our data collection process, we conduct multitrack music generation experiments evaluating how different representative subsets of PDMX lead to different behaviors in downstream models, and how user-rating statistics can be used as an effective measure of data quality. Examples can be found at https://pnlong.github.io/PDMX.demo/.'},\n",
       " {'title': 'Leveraging Structure and Context for Language-Adjacent Representation Learning',\n",
       "  'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:Y0pCki6q_DkC',\n",
       "  'authors': ['Nikita Srivatsan'],\n",
       "  'publication_date': '2024-04-04',\n",
       "  'description': 'When learning representations from large corpora of language data, the overwhelming strategy is to interpret that data as a collection of IID samples to be modeled in isolation from one another. While this approach is in some ways beneficial as it allows for efficient training via SGD and doesn’t rely on metadata that may not always be present, it does come with limitations. Taking advantage of more complex structural links between individual datapoints can let information flow within our corpora, making learned representations more context-sensitive, and allowing for heavier parameter sharing to more easily generalize to examples from unseen class types. In this work, we will apply this idea to a variety of settings—largely those that lie at the boundary between language and other modalities—for which much of the existing prior work has not explicitly made use of observable structure within the data, and also show how we can add useful inductive bias to our models through lower-level modeling choices. In order to retain interpretability and control we will do this both using probabilistic variational learning frameworks, and also non-variational approaches such as checklist models and retrieval guided generation.This dissertation is organized into three parts which apply this broad theme to various specific applications. First we’ll examine the task of learning disentangled representations of style and structure in digital fonts, and then apply similar modeling ideas to the task of analyzing handwriting styles of scribal hands of Linear B. In the next part we’ll investigate ways to learn contextualized representations for temporally ordered data where\\xa0…'},\n",
       " {'title': 'Generative AI for Music and Audio',\n",
       "  'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:Tyk-4Ss8FVUC',\n",
       "  'authors': ['Hao-Wen Dong'],\n",
       "  'publication_date': '2024-10-15',\n",
       "  'description': 'Generative AI has been transforming the way we interact with technology and consume content. In the next decade, AI technology will reshape how we create audio content in various media, including music, theater, films, games, podcasts, and short videos. In this dissertation, I introduce the three main directions of my research centered around generative AI for music and audio: 1) multitrack music generation, 2) assistive music creation tools, and 3) multimodal learning for audio and music. Through my research, I aim to answer the following two fundamental questions: 1) How can AI help professionals or amateurs create music and audio content? 2) Can AI learn to create music in a way similar to how humans learn music? My long-term goal is to lower the barrier of entry for music composition and democratize audio content creation.'},\n",
       " {'title': 'Unsupervised Lead Sheet Generation via Semantic Compression',\n",
       "  'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:2osOgNQ5qMEC',\n",
       "  'authors': ['Zachary Novack',\n",
       "   'Nikita Srivatsan',\n",
       "   'Taylor Berg-Kirkpatrick',\n",
       "   'Julian McAuley'],\n",
       "  'publication_date': '2023-10-16',\n",
       "  'description': 'Lead sheets have become commonplace in generative music research, being used as an initial compressed representation for downstream tasks like multitrack music generation and automatic arrangement. Despite this, researchers have often fallen back on deterministic reduction methods (such as the skyline algorithm) to generate lead sheets when seeking paired lead sheets and full scores, with little attention being paid toward the quality of the lead sheets themselves and how they accurately reflect their orchestrated counterparts. To address these issues, we propose the problem of conditional lead sheet generation (i.e. generating a lead sheet given its full score version), and show that this task can be formulated as an unsupervised music compression task, where the lead sheet represents a compressed latent version of the score. We introduce a novel model, called Lead-AE, that models the lead sheets as a discrete subselection of the original sequence, using a differentiable top-k operator to allow for controllable local sparsity constraints. Across both automatic proxy tasks and direct human evaluations, we find that our method improves upon the established deterministic baseline and produces coherent reductions of large multitrack scores.'},\n",
       " {'title': 'Down the Rabbit Hole: Modeling Twitter Dynamics through Bayesian Inference',\n",
       "  'link': '/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:u5HHmVD_uO8C',\n",
       "  'authors': ['Zachary Novak'],\n",
       "  'description': 'Social media usage, and its impact on people’s physical and mental health, is of interest to a diverse range of academic disciplines and everyday people. Despite this, we know very little about the ways in which, over months and years, someone’s social media use may escalate to consume significant amounts of their daily leisure time. Nor do we understand the ways in which a user’s posts may shift into toxic or unexpectedly abusive patterns. Understanding the long-term dynamics of use is complicated by the fact that day-to-day engagement has significantly non-normal statistics and may fluctuate by orders of magnitude—informally, users are sometimes driven to rare “binges” with lasting consequences for their future trajectory. To address this complex interplay of timescales, this work presents a Bayesian model for usage over time, flexible enough to capture a wide range of short and long-term temporal dependencies. Examining the “dose response” curves of a random sample of 500 users, we find that most users (≈ 90%) show evidence for a stable, equilibrium level of use. A smaller “high-risk” subset (≈ 10%) show evidence for instability: when short-term fluctuations drive their levels of use sufficiently high, they enter a new phase of sustained, run-away usage. Once we control for the levels of use, we find that “likes”, retweets, and other forms of feedback received from other users do not significantly impact future behavior. This casts doubt on the common heuristic that social media use is driven by an “addiction to likes\": for example, there is no evidence that a user whose posts receive an unexpectedly low level of likes posts more to “make\\xa0…'}]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "for work in works:\n",
    "    work_page = read_page_and_get_soup(base_url + work['link'])\n",
    "    for work_field in work_page.find(id=page_layout_tags['work_info_table']).findAll('div', recursive=False):\n",
    "        field_name = work_field.find('div', {'class': page_layout_tags['field_name']}).text.lower().replace(' ', '_')\n",
    "        field_value = work_field.find('div', {'class': page_layout_tags['field_value']})\n",
    "        \n",
    "        if field_name in valid_work_fields:\n",
    "            work[field_name] = field_processors[field_name](field_value)\n",
    "    time.sleep(2)\n",
    "\n",
    "works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"title\": \"Chils: Zero-shot image classification with hierarchical label sets\", \"link\": \"/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:9yKSN-GCB0IC\", \"authors\": [\"Zachary Novack\", \"Julian McAuley\", \"Zachary Chase Lipton\", \"Saurabh Garg\"], \"publication_date\": \"2023-07-03\", \"conference\": \"International Conference on Machine Learning\", \"pages\": \"26342-26362\", \"publisher\": \"PMLR\", \"description\": \"Open vocabulary models (eg CLIP) have shown strong performance on zero-shot classification through their ability generate embeddings for each class based on their (natural language) names. Prior work has focused on improving the accuracy of these models through prompt engineering or by incorporating a small amount of labeled downstream data (via finetuning). However, there has been little focus on improving the richness of the class names themselves, which can pose issues when class labels are coarsely-defined and are uninformative. We propose Classification with Hierarchical Label Sets (or CHiLS), an alternative strategy for zero-shot classification specifically designed for datasets with implicit semantic hierarchies. CHiLS proceeds in three steps:(i) for each class, produce a set of subclasses, using either existing label hierarchies or by querying GPT-3;(ii) perform the standard zero-shot CLIP procedure as though these subclasses were the labels of interest;(iii) map the predicted subclass back to its parent to produce the final prediction. Across numerous datasets with underlying hierarchical structure, CHiLS leads to improved accuracy in situations both with and without ground-truth hierarchical information. CHiLS is simple to implement within existing zero-shot pipelines and requires no additional training cost. Code is available at: https://github. com/acmi-lab/CHILS.\", \"total_citations\": 71}, {\"title\": \"Ditto: Diffusion inference-time t-optimization for music generation\", \"link\": \"/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:qjMakFHDy7sC\", \"authors\": [\"Zachary Novack\", \"Julian McAuley\", \"Taylor Berg-Kirkpatrick\", \"Nicholas J Bryan\"], \"publication_date\": \"2024-01-22\", \"description\": \"We propose Diffusion Inference-Time T-Optimization (DITTO), a general-purpose frame-work for controlling pre-trained text-to-music diffusion models at inference-time via optimizing initial noise latents. Our method can be used to optimize through any differentiable feature matching loss to achieve a target (stylized) output and leverages gradient checkpointing for memory efficiency. We demonstrate a surprisingly wide-range of applications for music generation including inpainting, outpainting, and looping as well as intensity, melody, and musical structure control - all without ever fine-tuning the underlying model. When we compare our approach against related training, guidance, and optimization-based methods, we find DITTO achieves state-of-the-art performance on nearly all tasks, including outperforming comparable approaches on controllability, audio quality, and computational efficiency, thus opening the door for high-quality, flexible, training-free control of diffusion models. Sound examples can be found at https://DITTO-Music.github.io/web/.\", \"total_citations\": 16}, {\"title\": \"Disentangling the Mechanisms Behind Implicit Regularization in SGD\", \"link\": \"/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:u-x6o8ySG0sC\", \"authors\": [\"Zachary Novack\", \"Simran Kaur\", \"Tanya Marwah\", \"Saurabh Garg\", \"Zachary C Lipton\"], \"publication_date\": \"2022-11-29\", \"description\": \"A number of competing hypotheses have been proposed to explain why small-batch Stochastic Gradient Descent (SGD)leads to improved generalization over the full-batch regime, with recent work crediting the implicit regularization of various quantities throughout training. However, to date, empirical evidence assessing the explanatory power of these hypotheses is lacking. In this paper, we conduct an extensive empirical evaluation, focusing on the ability of various theorized mechanisms to close the small-to-large batch generalization gap. Additionally, we characterize how the quantities that SGD has been claimed to (implicitly) regularize change over the course of training. By using micro-batches, i.e. disjoint smaller subsets of each mini-batch, we empirically show that explicitly penalizing the gradient norm or the Fisher Information Matrix trace, averaged over micro-batches, in the large-batch regime recovers small-batch SGD generalization, whereas Jacobian-based regularizations fail to do so. This generalization performance is shown to often be correlated with how well the regularized model\\'s gradient norms resemble those of small-batch SGD. We additionally show that this behavior breaks down as the micro-batch size approaches the batch size. Finally, we note that in this line of inquiry, positive experimental findings on CIFAR10 are often reversed on other datasets like CIFAR100, highlighting the need to test hypotheses on a wider collection of datasets.\", \"total_citations\": 2}, {\"title\": \"Futga: Towards Fine-grained Music Understanding through Temporally-enhanced Generative Augmentation\", \"link\": \"/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:IjCSPb-OGe4C\", \"authors\": [\"Junda Wu\", \"Zachary Novack\", \"Amit Namburi\", \"Jiaheng Dai\", \"Hao-Wen Dong\", \"Zhouhang Xie\", \"Carol Chen\", \"Julian McAuley\"], \"publication_date\": \"2024-07-29\", \"description\": \"Existing music captioning methods are limited to generating concise global descriptions of short music clips, which fail to capture fine-grained musical characteristics and time-aware musical changes. To address these limitations, we propose FUTGA, a model equipped with fined-grained music understanding capabilities through learning from generative augmentation with temporal compositions. We leverage existing music caption datasets and large language models (LLMs) to synthesize fine-grained music captions with structural descriptions and time boundaries for full-length songs. Augmented by the proposed synthetic dataset, FUTGA is enabled to identify the music\\'s temporal changes at key transition points and their musical functions, as well as generate detailed descriptions for each music segment. We further introduce a full-length music caption dataset generated by FUTGA, as the augmentation of the MusicCaps and the Song Describer datasets. We evaluate the automatically generated captions on several downstream tasks, including music generation and retrieval. The experiments demonstrate the quality of the generated captions and the better performance in various downstream tasks achieved by the proposed music captioning approach. Our code and datasets can be found in \\\\\\\\href{https://huggingface.co/JoshuaW1997/FUTGA}{\\\\\\\\textcolor{blue}{https://huggingface.co/JoshuaW1997/FUTGA}}.\", \"total_citations\": 1}, {\"title\": \"DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation\", \"link\": \"/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:UeHWp8X0CEIC\", \"authors\": [\"Zachary Novack\", \"Julian McAuley\", \"Taylor Berg-Kirkpatrick\", \"Nicholas Bryan\"], \"publication_date\": \"2024-05-30\", \"description\": \"Controllable music generation methods are critical for human-centered AI-based music creation, but are currently limited by speed, quality, and control design trade-offs. Diffusion Inference-Time T-optimization (DITTO), in particular, offers state-of-the-art results, but is over 10x slower than real-time, limiting practical use. We propose Distilled Diffusion Inference-Time T -Optimization (or DITTO-2), a new method to speed up inference-time optimization-based control and unlock faster-than-real-time generation for a wide-variety of applications such as music inpainting, outpainting, intensity, melody, and musical structure control. Our method works by (1) distilling a pre-trained diffusion model for fast sampling via an efficient, modified consistency or consistency trajectory distillation process (2) performing inference-time optimization using our distilled model with one-step sampling as an efficient surrogate optimization task and (3) running a final multi-step sampling generation (decoding) using our estimated noise latents for best-quality, fast, controllable generation. Through thorough evaluation, we find our method not only speeds up generation over 10-20x, but simultaneously improves control adherence and generation quality all at once. Furthermore, we apply our approach to a new application of maximizing text adherence (CLAP score) and show we can convert an unconditional diffusion model without text inputs into a model that yields state-of-the-art text control. Sound examples can be found at https://ditto-music.github.io/ditto2/.\", \"total_citations\": 1}, {\"title\": \"Presto! Distilling Steps and Layers for Accelerating Music Generation\", \"link\": \"/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:YsMSGLbcyi4C\", \"authors\": [\"Zachary Novack\", \"Ge Zhu\", \"Jonah Casebeer\", \"Julian McAuley\", \"Taylor Berg-Kirkpatrick\", \"Nicholas J Bryan\"], \"publication_date\": \"2024-10-07\", \"description\": \"Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) -- the fastest high-quality TTM to our knowledge. Sound examples can be found at https://presto-music.github.io/web/.\"}, {\"title\": \"Deriving Representative Structure Over Music Corpora\", \"link\": \"/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:eQOLeE2rZwMC\", \"authors\": [\"Ilana Shapiro\", \"Ruanqianqian Huang\", \"Zachary Novack\", \"Cheng-I Wang\", \"Hao-Wen Dong\", \"Taylor Berg-Kirkpatrick\", \"Shlomo Dubnov\", \"Sorin Lerner\"], \"publication_date\": \"2024-10-05\", \"description\": \"Western music is an innately hierarchical system of interacting levels of structure, from fine-grained melody to high-level form. In order to analyze music compositions holistically and at multiple granularities, we propose a unified, hierarchical meta-representation of musical structure called the structural temporal graph (STG). For a single piece, the STG is a data structure that defines a hierarchy of progressively finer structural musical features and the temporal relationships between them. We use the STG to enable a novel approach for deriving a representative structural summary of a music corpus, which we formalize as a dually NP-hard combinatorial optimization problem. Our approach first applies simulated annealing to develop a measure of structural distance between two music pieces rooted in graph isomorphism. Our approach then combines the formal guarantees of SMT solvers with nested simulated annealing over structural distances to produce a structurally sound, representative centroid STG for an entire corpus of STGs obtained from individual pieces. To evaluate our approach, we conduct experiments showing that structural distance accurately differentiates between music pieces, and that our computed centroids encapsulate the overarching structure of a music corpus.\"}, {\"title\": \"CoLLAP: Contrastive Long-form Language-Audio Pretraining with Musical Temporal Structure Augmentation\", \"link\": \"/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:W7OEmFMy1HYC\", \"authors\": [\"Junda Wu\", \"Warren Li\", \"Zachary Novack\", \"Amit Namburi\", \"Carol Chen\", \"Julian McAuley\"], \"publication_date\": \"2024-10-03\", \"description\": \"Modeling temporal characteristics plays a significant role in the representation learning of audio waveform. We propose Contrastive Long-form Language-Audio Pretraining (\\\\\\\\textbf{CoLLAP}) to significantly extend the perception window for both the input audio (up to 5 minutes) and the language descriptions (exceeding 250 words), while enabling contrastive learning across modalities and temporal dynamics. Leveraging recent Music-LLMs to generate long-form music captions for full-length songs, augmented with musical temporal structures, we collect 51.3K audio-text pairs derived from the large-scale AudioSet training dataset, where the average audio length reaches 288 seconds. We propose a novel contrastive learning architecture that fuses language representations with structured audio representations by segmenting each song into clips and extracting their embeddings. With an attention mechanism, we capture multimodal temporal correlations, allowing the model to automatically weigh and enhance the final fusion score for improved contrastive alignment. Finally, we develop two variants of the CoLLAP model with different types of backbone language models. Through comprehensive experiments on multiple long-form music-text retrieval datasets, we demonstrate consistent performance improvement in retrieval accuracy compared with baselines. We also show the pretrained CoLLAP models can be transferred to various music information retrieval tasks, with heterogeneous long-form multimodal contexts.\"}, {\"title\": \"PDMX: A Large-Scale Public Domain MusicXML Dataset for Symbolic Music Processing\", \"link\": \"/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:zYLM7Y9cAGgC\", \"authors\": [\"Phillip Long\", \"Zachary Novack\", \"Taylor Berg-Kirkpatrick\", \"Julian McAuley\"], \"publication_date\": \"2024-09-17\", \"description\": \"The recent explosion of generative AI-Music systems has raised numerous concerns over data copyright, licensing music from musicians, and the conflict between open-source AI and large prestige companies. Such issues highlight the need for publicly available, copyright-free musical data, in which there is a large shortage, particularly for symbolic music data. To alleviate this issue, we present PDMX: a large-scale open-source dataset of over 250K public domain MusicXML scores collected from the score-sharing forum MuseScore, making it the largest available copyright-free symbolic music dataset to our knowledge. PDMX additionally includes a wealth of both tag and user interaction metadata, allowing us to efficiently analyze the dataset and filter for high quality user-generated scores. Given the additional metadata afforded by our data collection process, we conduct multitrack music generation experiments evaluating how different representative subsets of PDMX lead to different behaviors in downstream models, and how user-rating statistics can be used as an effective measure of data quality. Examples can be found at https://pnlong.github.io/PDMX.demo/.\"}, {\"title\": \"Leveraging Structure and Context for Language-Adjacent Representation Learning\", \"link\": \"/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:Y0pCki6q_DkC\", \"authors\": [\"Nikita Srivatsan\"], \"publication_date\": \"2024-04-04\", \"description\": \"When learning representations from large corpora of language data, the overwhelming strategy is to interpret that data as a collection of IID samples to be modeled in isolation from one another. While this approach is in some ways beneficial as it allows for efficient training via SGD and doesn\\\\u2019t rely on metadata that may not always be present, it does come with limitations. Taking advantage of more complex structural links between individual datapoints can let information flow within our corpora, making learned representations more context-sensitive, and allowing for heavier parameter sharing to more easily generalize to examples from unseen class types. In this work, we will apply this idea to a variety of settings\\\\u2014largely those that lie at the boundary between language and other modalities\\\\u2014for which much of the existing prior work has not explicitly made use of observable structure within the data, and also show how we can add useful inductive bias to our models through lower-level modeling choices. In order to retain interpretability and control we will do this both using probabilistic variational learning frameworks, and also non-variational approaches such as checklist models and retrieval guided generation.This dissertation is organized into three parts which apply this broad theme to various specific applications. First we\\\\u2019ll examine the task of learning disentangled representations of style and structure in digital fonts, and then apply similar modeling ideas to the task of analyzing handwriting styles of scribal hands of Linear B. In the next part we\\\\u2019ll investigate ways to learn contextualized representations for temporally ordered data where\\\\u00a0\\\\u2026\"}, {\"title\": \"Generative AI for Music and Audio\", \"link\": \"/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:Tyk-4Ss8FVUC\", \"authors\": [\"Hao-Wen Dong\"], \"publication_date\": \"2024-10-15\", \"description\": \"Generative AI has been transforming the way we interact with technology and consume content. In the next decade, AI technology will reshape how we create audio content in various media, including music, theater, films, games, podcasts, and short videos. In this dissertation, I introduce the three main directions of my research centered around generative AI for music and audio: 1) multitrack music generation, 2) assistive music creation tools, and 3) multimodal learning for audio and music. Through my research, I aim to answer the following two fundamental questions: 1) How can AI help professionals or amateurs create music and audio content? 2) Can AI learn to create music in a way similar to how humans learn music? My long-term goal is to lower the barrier of entry for music composition and democratize audio content creation.\"}, {\"title\": \"Unsupervised Lead Sheet Generation via Semantic Compression\", \"link\": \"/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:2osOgNQ5qMEC\", \"authors\": [\"Zachary Novack\", \"Nikita Srivatsan\", \"Taylor Berg-Kirkpatrick\", \"Julian McAuley\"], \"publication_date\": \"2023-10-16\", \"description\": \"Lead sheets have become commonplace in generative music research, being used as an initial compressed representation for downstream tasks like multitrack music generation and automatic arrangement. Despite this, researchers have often fallen back on deterministic reduction methods (such as the skyline algorithm) to generate lead sheets when seeking paired lead sheets and full scores, with little attention being paid toward the quality of the lead sheets themselves and how they accurately reflect their orchestrated counterparts. To address these issues, we propose the problem of conditional lead sheet generation (i.e. generating a lead sheet given its full score version), and show that this task can be formulated as an unsupervised music compression task, where the lead sheet represents a compressed latent version of the score. We introduce a novel model, called Lead-AE, that models the lead sheets as a discrete subselection of the original sequence, using a differentiable top-k operator to allow for controllable local sparsity constraints. Across both automatic proxy tasks and direct human evaluations, we find that our method improves upon the established deterministic baseline and produces coherent reductions of large multitrack scores.\"}, {\"title\": \"Down the Rabbit Hole: Modeling Twitter Dynamics through Bayesian Inference\", \"link\": \"/citations?view_op=view_citation&hl=en&oe=ASCII&user=fZKJdb0AAAAJ&citation_for_view=fZKJdb0AAAAJ:u5HHmVD_uO8C\", \"authors\": [\"Zachary Novak\"], \"description\": \"Social media usage, and its impact on people\\\\u2019s physical and mental health, is of interest to a diverse range of academic disciplines and everyday people. Despite this, we know very little about the ways in which, over months and years, someone\\\\u2019s social media use may escalate to consume significant amounts of their daily leisure time. Nor do we understand the ways in which a user\\\\u2019s posts may shift into toxic or unexpectedly abusive patterns. Understanding the long-term dynamics of use is complicated by the fact that day-to-day engagement has significantly non-normal statistics and may fluctuate by orders of magnitude\\\\u2014informally, users are sometimes driven to rare \\\\u201cbinges\\\\u201d with lasting consequences for their future trajectory. To address this complex interplay of timescales, this work presents a Bayesian model for usage over time, flexible enough to capture a wide range of short and long-term temporal dependencies. Examining the \\\\u201cdose response\\\\u201d curves of a random sample of 500 users, we find that most users (\\\\u2248 90%) show evidence for a stable, equilibrium level of use. A smaller \\\\u201chigh-risk\\\\u201d subset (\\\\u2248 10%) show evidence for instability: when short-term fluctuations drive their levels of use sufficiently high, they enter a new phase of sustained, run-away usage. Once we control for the levels of use, we find that \\\\u201clikes\\\\u201d, retweets, and other forms of feedback received from other users do not significantly impact future behavior. This casts doubt on the common heuristic that social media use is driven by an \\\\u201caddiction to likes\\\\\": for example, there is no evidence that a user whose posts receive an unexpectedly low level of likes posts more to \\\\u201cmake\\\\u00a0\\\\u2026\"}]'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('zach_scrape.json', 'w') as out_file:\n",
    "    json.dump(works, out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
